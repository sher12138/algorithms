## MP：具有马尔科夫性质的随机过程

*mp: 随机过程， 某时刻状态只取决于上一时刻的状态* 

*mp*:  使用 $\ S, P$
- $S$: 状态空间，有限状态集合。
$$
\mathcal{P}=\begin{bmatrix}P(s_1|s_1)&\cdots&P(s_n|s_1)\\\vdots&\ddots&\vdots\\P(s_1|s_n)&\cdots&P(s_n|s_n)\end{bmatrix}
$$


*mrp*: 马尔科夫奖励过程，使用 $\ S, P, R, \gamma$

- $R$: 奖励函数，$R(s) = E[R_{t+1} | S_t = s]$  转移到该状态的奖励期望值
- $\gamma$: 折扣因子，$0 \leq \gamma \leq 1$，未来奖励的重要性 

*回报*：从第t时刻的状态开始，直到终止奖励时，所有奖励的衰减之和称为回报。

- $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$

*价值函数*：一个状态的期望回报，所有状态的价值组成价值函数。 

$$
\begin{aligned}
V(s)& =E[G_t|S_t=s] \\
&=E[R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+\cdots|S_t=s] \\
&=E[R_t+\gamma(R_{t+1}+\gamma R_{t+2}+\cdots)|S_t=s] \\
&=E[R_t+\gamma G_{t+1}|S_t=s] \\
&=E[R_t+\gamma V(S_{t+1})|S_t=s]
\end{aligned}
$$

对期望做简化，得到Bellman方程：

$$
V(s) = r_{t+1} + \sum_{s^{\prime} \in S} \gamma P(s^{\prime} | s) V(s^{\prime})
$$

表示成矩阵的形式：
$$
\begin{gathered}\mathcal{V}=\mathcal{R}+\gamma\mathcal{PV}\\\begin{bmatrix}V(s_1)\\V(s_2)\\\vdots\\V(s_n)\end{bmatrix}=\begin{bmatrix}r(s_1)\\r(s_2)\\\vdots\\r(s_n)\end{bmatrix}+\gamma\begin{bmatrix}P(s_1|s_1)&\cdots&P(s_n|s_1)\\P(s_1|s_2)&\cdots&P(s_n|s_2)\\\vdots&\ddots&\vdots\\P(s_1|s_n)&\cdots&P(s_n|s_n)\end{bmatrix}\begin{bmatrix}V(s_1)\\V(s_2)\\\vdots\\V(s_n)\end{bmatrix}\end{gathered}
$$

解析解：
$$
\mathcal{V}=(\mathcal{I}-\gamma\mathcal{P})^{-1}\mathcal{R}
$$

式中，
- $\mathcal{I}$是单位矩阵
- $\mathcal{P}$是状态转移概率矩阵
- $\mathcal{R}$是奖励矩阵
- $\mathcal{V}$是价值函数矩阵
- $\gamma$是折扣因子

*mdp*: 马尔科夫决策过程，使用 $\ S, A, P, R, \gamma$

- P: 状态转移概率，$P(s^{\prime} | s, a) = P(S_{t+1} = s^{\prime} | S_t = s, A_t = a)$

*策略*：一个从状态到动作的映射，$\pi(a|s) = P(A_t = a | S_t = s)$


*状态价值函数or策略价值函数*：状态价值函数是在状态s下，按照策略$\pi$所得到的期望回报。

$$
\begin{aligned}
v(s,\pi) &= E[G_t | S_t = s] \\
&=E(r_{t+1} + \gamma G_{t+1} | S_t = s)  \\
&=E(r_{t+1} | S_t = s) + \gamma E(G_{t+1} | S_t = s)
\end{aligned}
$$

两部分单独计算：
$$
\begin{aligned}
E(r_{t+1} | S_t = s) &= \sum_{a} \pi(a|s) \sum_{s^{\prime},r} p(s^{\prime},r|s,a) r\\
E(G_{t+1} | S_t = s) &= \sum_{a} \pi(a|s) \sum_{s^{\prime},r} p(s^{\prime},r|s,a) v(s^{\prime},\pi)
\end{aligned}
$$


*动作价值函数*：在状态s下，执行动作a，再按照策略$\pi$所得到的期望回报。动作存在导致的新价值形式。

$$
q(s,a,\pi) = E(G_t | S_t = s, A_t = a)
$$

二者关系：

$$
q(s,a,\pi) = \sum_r p(r | s,a) r + \gamma \sum_{s^{\prime}} p(s^{\prime} | s, a) v(s^{\prime}, \pi)
$$

*贝尔曼期望方程*：

$$\begin{aligned}
Q^{\pi}(s,a)& =E_\pi[G_t|S_t=s,A_t=a] \\
&=r(s,a)+\gamma\sum_{s'\in S}P(s'|s,a)V^{\pi}(s') \\
&=r(s,a)+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}|s,a)\sum_{a^{\prime}\in\mathcal{A}}\pi(a^{\prime}|s^{\prime})Q^\pi(s^{\prime},a^{\prime}) \\
V^{\pi}(s)& =E_\pi[G_t|S_t=s] \\
&=\sum_{a\in\mathcal{A}}\pi(a|s)Q^\pi(s,a) \\
&=\sum_{a\in\mathcal{A}}\pi(a|s)[r(s,a)+\gamma\sum_{s^{\prime}\in S}P(s^{\prime}|s,a)V^{\pi}(s^{\prime})]
\end{aligned}$$

*最优策略*：使得价值函数最大的策略。

最优价值函数：
$$
V^*(s) = \max_{\pi} V^{\pi}(s)
$$

最优动作价值函数：
$$
Q^*(s,a) = \max_{\pi} Q^{\pi}(s,a)
$$

*最优贝尔曼方程*：

$$
V^*(s) = \max_{a} \sum_{s^{\prime}} P(s^{\prime} | s, a) [r(s,a) + \gamma V^*(s^{\prime})]
$$

$$
Q^*(s,a) = \sum_{s^{\prime}} P(s^{\prime} | s, a) [r(s,a) + \gamma \max_{a^{\prime}} Q^*(s^{\prime}, a^{\prime})]
$$

### 求解算法

#### 动态规划

*策略迭代*
- 策略评估：迭代计算状态价值函数
$$
V_{k+1}(s) = \sum_{a} \pi(a|s) \sum_{s^{\prime},r} p(s^{\prime},r|s,a) [r + \gamma V_k(s^{\prime})]
$$
- 策略改进：根据状态价值函数，更新策略
$$
\pi^{\prime}(a|s) = \arg \max_{a} \sum_{s^{\prime},r} p(s^{\prime},r|s,a) [r + \gamma V(s^{\prime})]
$$

*值迭代*
- 价值迭代：迭代计算最优价值函数
$$
V_{k+1}(s) = \max_{a} \sum_{s^{\prime},r} p(s^{\prime},r|s,a) [r + \gamma V_k(s^{\prime})]
$$

- 策略提取：根据最优价值函数，提取最优策略