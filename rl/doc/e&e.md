# 探索和利用

## 1. 介绍

### MCTS
估计值函数的前提是能够合理的访问每个可能的状态。在状态空间很大的情况下，很难访问到每个状态。


## 2. 探索策略

### 2.1 $\epsilon$-贪心策略
$\epsilon$-贪心策略是一种简单的探索策略，以$\epsilon$的概率随机选择动作，以$1-\epsilon$的概率选择当前最优的动作。

*问题*：$\epsilon$ 概率均匀访问，不考虑状态的重要性，不考虑状态的价值。

### 2.2 UCB
UCB是一种基于置信上界的探索策略，它的核心思想是在不确定性较大的状态上增加探索的概率。

$$
A_t = \arg \max_{a} [\frac{Q(s, a)}{N(s,a)} + c \sqrt{\frac{\ln N(s)}{N(s, a)}}]
$$

其中，$Q(s, a)$是状态-动作值函数，$N(s, a)$是状态-动作的访问次数，$N(s)$是状态的访问次数，$c$是一个常数。

### 2.3 异策略MCTS

将评估 和 行动分开，评估使用一个策略，行动使用另一个策略。