# 强化学习


 ## 1. 马尔科夫决策过程

 ### 1.1 马尔科夫性质 

    马尔科夫性质是指一个随机过程，如果已知当前状态，那么过去的状态对于预测未来的状态没有任何意义。即未来的状态只与当前状态有关，与过去的状态无关。
$$
P(S_{t+1}|S_t) = P(S_{t+1}|S_t, S_{t-1}, S_{t-2}, \cdots, S_1)
$$

引入动作 $a$
$$
P(S_{t+1}|S_t, a_t) = P(S_{t+1}|S_t, a_t, S_{t-1}, a_{t-1}, S_{t-2}, a_{t-2}, \cdots, S_1, a_1)
$$

状态非完全可观测

*HMM 隐马尔科夫模型*


*POMDP 部分可观测马尔科夫决策过程*

#### 数学形式 
$$
\begin{align*}
&状态 s  \\
&动作 a \\
&状态转移概率 P(s'|s, a) \\ 
&奖励函数 R(s, a, s') 
\end{align*}
$$

智能体在时间步t时决策的目标就是使得之后的累计奖励最大，即最大化累计奖励 
$$
R_t  = \sum_{t=0}^{\infty} \gamma^t R_{t+1}
$$

### 1.2 值函数 
某个状态s可以获得的长期回报的期望值，状态值函数 
$$
V(s) = E[R_t|s_t = s]
$$ 

某个状态s采取某个动作a后，可以获得的长期回报的期望值，状态-动作值函数
$$
Q(s, a) = E[R_t|s_t = s, a_t = a]
$$

### 1.3 策略
策略是智能体在每个状态下选择动作的概率分布，策略函数
$$
\pi(a|s) = P(a_t = a|s_t = s)
$$

### 1.4 策略的值函数 
$$
V_{\pi}(s) = E_{\pi}[R_t|s_t = s] \\
Q_{\pi}(s, a) = E_{\pi}[R_t|s_t = s, a_t = a]
$$

### 1.5 贝尔曼方程


状态 $s$的$v$值，等于它在该状态下做所有可能动作的$Q$值的概率加权和

状态$s$下做出动作$a$的$Q$值，等于它在该状态下做出动作$a$后的奖励，加上下一个状态的$v$值
$$
V_{\pi}(s) = \sum_{a} \pi(a|s) q_{\pi}(s,a) \\ 
Q_{\pi}(s, a) = \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma v_{\pi}(s')]
$$ 

*贝尔曼方程*：
$$
V_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma V_{\pi}(s_{t+1})|s_t = s] \\
Q_{\pi}(s, a) = E_{\pi}[R_{t+1} + \gamma Q_{\pi}(s_{t+1}, a_{t+1})|s_t = s, a_t = a]
$$ 

*贝尔曼最优方程* ：如果假定最优策略，使总的收益最大，即值函数最大。因此我们假定最优的策略$\pi^*$，使得值函数最大 
$$
V_{\pi^*}(s) = \max_{\pi} V_{\pi}(s) \\
Q_{\pi^*}(s, a) = \max_{\pi} Q_{\pi}(s, a)
$$

### 1.6 策略迭代与值迭代

**策略迭代**：先初始化一个策略，然后根据当前策略计算出值函数，然后根据值函数计算出最优策略，然后根据最优策略计算出值函数，如此循环迭代，直到策略不再变化。
1. 初始化策略$\pi_0$
2. 循环迭代
    1. 根据当前策略计算值函数$V_{\pi_i}$
    $$
    V_{\pi_i}^{i+1}(s) = \sum_{s'} P(s'|s, \pi_i(s))[R(s, \pi_i(s), s') + \gamma V_{\pi_i}(s')] 
      $$
    2. 根据值函数计算最优策略$\pi_{i+1}$
    $$
    \pi_{i+1}(s) = \arg \max_{a} \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma V_{\pi_i}(s')]
    $$
    3. 如果$\pi_{i+1} = \pi_i$，则停止迭代

**值迭代**：先初始化一个值函数，然后根据当前值函数计算出最优策略，然后根据最优策略计算出值函数，如此循环迭代，直到值函数不再变化。
1. 初始化值函数$V_0$
2. 循环迭代
    1. 根据当前值函数计算最优策略$\pi_i$  
    $$
    \pi_i(s) = \arg \max_{a} \sum_{s'} P(s'|s, a)[R(s, a, s') + \gamma V_i(s')]
    $$
    2. 根据最优策略计算值函数$V_{i+1}$
    3. 如果$V_{i+1} = V_i$，则停止迭代