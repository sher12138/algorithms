# 蒙特卡洛


## 1. 蒙特卡洛方法求 v 函数

初始状态 $s_0$，根据策略 $\pi$ 采样，得到一条轨迹，同时获得一个奖励$r_t$，直到终止状态 $s_T$。然后根据这条轨迹，计算每个状态的累计奖励，然后求平均值，得到状态值函数 $V(s)$。

$$
V(s) = \frac{1}{N(s)} \sum_{i=1}^{N(s)} G_i(s)
$$

**首次访问MC方法**：只有当状态 $s$ 第一次出现在轨迹中，才计算 $G_i(s)$，否则不计算。

**每次访问MC方法**：每次状态 $s$ 出现在轨迹中，都计算 $G_i(s)$。

## 2. 蒙特卡洛方法求 q 函数

初始状态 $s_0$，根据策略 $\pi$ 采样，得到一条轨迹，同时获得一个奖励$r_t$，直到终止状态 $s_T$。然后根据这条轨迹，计算每个状态-动作的累计奖励，然后求平均值，得到状态-动作值函数 $Q(s, a)$。

在直到状态 $s$ 的 $v$ 值的情况下，策略：
$$
\pi(s) = \arg \max_{a} Q(s, a)  \\
Q(s,a) = r(s,a) + \gamma \sum_{s'} P(s'|s,a) V(s')
$$ 

不知道 $P(s'|s,a)$ ,不知道 $r(s,a)$。

如果能直接获得 $Q(s,a)$ ，则可以直接得到最优策略。
